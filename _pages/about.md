---
permalink: / 
layout: archive
title: "Homepage"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

**"Think big, act small, and move fast"**

This webpage is last updated on **2025/04/05**.

ƒê·ªÉ xem phi√™n b·∫£n ti·∫øng Vi·ªát c·ªßa trang n√†y, vui l√≤ng k√©o xu·ªëng d∆∞·ªõi ho·∫∑c nh·∫•n v√†o [ƒë√¢y](https://dovanquyet.github.io/#trang-ch·ªß-phi√™n-b·∫£n-ti·∫øng-vi·ªát-cho-trang-n√†y)

<!-- > I am open to supervising UG students who are academically strong, highly motivated, and having a decent AI-coding skill (Python, Pytorch, and Huggingface Transformers are preferred). If you are interested in NLP and want to collaborate with me, feel free to contact and send your CV + Unofficial Transcript to me via email.  -->

## About me

Hi folks, welcome to my personal homepage! You can call me by my real name Quyet (pronounce like 'Quest'), or my English name William. I'm a first-year PhD student at Virginia Tech where I work with [Prof. Tu Vu](https://tuvllms.github.io/). Previously, I got my M.Phil in Computer Science and B.Sc in Data Science and Pure Math (Advanced Track) from HKUST in Sep 2024 and July 2022. During my MPhil program, I was fortunate to be supervised by [Prof. Yangqiu Song](https://www.cse.ust.hk/~yqsong/).

**My primary research interests are centered around AI/NLP** (Artificial Intelligence / Natural Language Processing). Although the field drastically progresses in the last decade, I do not expect language models (as the "main products" of the field) to really understand human language nor expose super-human capability. Instead, I expect them to mimic human behaviors (either as an individual or a group of people) as realistic as possible and benefit industrial applications.

I aspire to develop artificial intelligence (AI) in a similar way as human intelligence develops. In the era of large AI models, my research focus includes:

1. **Instruction following capability**: At this moment, State-of-The-Art LLMs can perform well on multiple tasks with simple instructions. But what if the task and instructions are more realistic and complex? Can LLMs follow all the instructions? That is what I am working on.
1. **Model merging**: It is very impactful if we can merge models (of difference size, different pretraining procedure) so that we can transfer strengths and minimize weaknesses of models in the merged model.
1. **AI for Math**: As an IMO medalist, I am interested in developing AI math-reasoning systems based on my math-solving techniques.
1. **Neuro-symbolic reasoning system**: Learning both system-1 and sytem-2 reasoning capability in an efficient way and within a model by combining neural and symbolic modules.
<!-- 1. Neuro-symbolic reasoning methods: I intend to investigate the utilization of external resources, specifically existing knowledge graphs, in order to facilitate the identification and rectification of erroneous beliefs within language models. "RECKONING" and "Language Models with Rationality" -->
<!-- 1. Efficient (transfer) learning: I aspire to enable LMs to learn from very little supervision, which is a hallmark of human intelligence. I am particularly interested in how to discover and effectively finetune the most critical subnetworks of LMs w.r.t to each task. RS_Tu.pdf and "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models" -->
<!-- 1. Basic learning ability: How to train and evaluate LLMs on such crucial skills (rule following (shift of distribution), new skill (soft rule)) -->

About my personal life, I am a big fan of music. I consider myself as an amateur singer. Check this [Youtube channel](https://www.youtube.com/channel/UCw0K4xQPwp8wZp6rkWRcTCg) to hear my voice :D Besides, about my spiritual, I am hugely inspired by the [Zen Master Thich Nhat Hanh](https://plumvillage.org/thich-nhat-hanh/). His Buddhism accounts for the major portion of my reasoning system. My favorite book, not surprisingly, is one of his books, Old Path White Cloud ([en](https://terebess.hu/zen/mesterek/Thich%20Nhat%20Hanh%20-%20Old%20Path%20White%20Clouds.pdf),[vi](https://thuvienhoasen.org/images/file/3GfDvp1G0QgQAHtP/duong-xua-may-trang.pdf)). Also, from Feb 2023 to April 2024, I served as the Head of Event Organization of [Vietnamese Students' Association in HK](https://www.facebook.com/profile.php?id=100087606602683).

My full academic profile can be found [here](https://dovanquyet.github.io/academic). For (interesting) stories about my personal life, please check [here](https://dovanquyet.github.io/posts/vi/chuyen-hang-ngay).


## News

- [2025/05] Start my Summer Internship at Adobe Research üéâ
- [2024/08] Start my PhD journey at Virginia Tech under the supervision of Prof. Tu Vu
- [2024/03] Decide to do my PhD at Virginia Tech!
- [2024/01] My first first-author [paper](https://arxiv.org/abs/2401.14003) has been accepted to EACL'24!
- [2024/01] Finished my PhD application. I got 6 interviews from 4 universities :D But probably no more :/ Updated: two more interviews, one is from a university that I haven't applied :D And later one more interview invitation but I declined due my crazily heavy workload :/
- [2023/05] Our paper on Contextualized Commonsense Causal Reasoning has been accepted to ACL'23 Main Conference. Check it [here](https://arxiv.org/abs/2305.05191)
- [2023/04] Our paper on Commonsense Knowledge Base Population is archived. Check it [here](https://arxiv.org/abs/2304.10392)
- [2023/02] Our work on evaluating ChatGPT is out. Check this [paper](https://arxiv.org/abs/2302.04023). Updated: it is accepted to AACL 2023 (Poster).
- [2022/11] Have a chat with Prof. Pascale. She is also interested in using commonsense knowledge to control the language model's generation and support open-ended QA. Thus, I start working with her students :P. Updated: I am no longer a part of that group since July 2023. I feel in debt that I learned a lot from all people in the lab but only contributed a little :/
- [2022/10] My [first paper](https://arxiv.org/abs/2210.07988) has been accepted as Findings of EMNLP2022. Yeah!!!
- [2022/06] My first paper has been submitted to EMNLP2022. Let's see if it's accepted =)
- [2022/05] My application for M.Phil program to HKUST is accepted. I will remain in HK for two more years.


## Contact me!

Feel free to reach me at {my first name in the webpage's title}{my last name} {at} vt [dot] edu.


## Trang Ch·ªß (phi√™n b·∫£n ti·∫øng Vi·ªát cho trang n√†y)

Ch√†o m·ª´ng m·ªçi ng∆∞·ªùi ƒë·∫øn m·ªõi trang ch·ªß c·ªßa m√¨nh. Hi·ªán t·∫°i m√¨nh ƒëang l√† nghi√™n c·ª©u sinh nƒÉm nh·∫•t c·ªßa ch∆∞∆°ng tr√¨nh Ti·∫øn Sƒ© ·ªü ƒê·∫°i h·ªçc B√°ch Khoa bang Virginia (Virginia Tech) v·ªõi s·ª± h∆∞·ªõng d·∫´n c·ªßa Gi√°o s∆∞ [V≈© Thanh T√∫](https://tuvllms.github.io/). Tr∆∞·ªõc ƒë√≥, m√¨nh nh·∫≠n b·∫±ng Th·∫°c sƒ© Nghi√™n c·ª©u (MPhil) ng√†nh Computer Science (Khoa h·ªçc M√°y t√≠nh) v√† C·ª≠ nh√¢n ng√†nh Data Science (Khoa h·ªçc D·ªØ li·ªáu) v√† Advanced Pure Math (To√°n thu·∫ßn t√∫y n√¢ng cao) t·ª´ tr∆∞·ªùng ·ªü [Hong Kong University of Science and Technology](https://hkust.edu.hk/) v√†o th√°ng 9 nƒÉm 2024 v√† th√°ng 7 nƒÉm 2022. Trong th·ªùi gian h·ªçc Th·∫°c sƒ©, m√¨nh may m·∫Øn ƒë∆∞·ª£c l√†m vi·ªác v·ªõi Gi√°o s∆∞ [Yangqiu Song (T·ªëng D∆∞∆°ng Thu)](https://www.cse.ust.hk/~yqsong/).

Nghi√™n c·ª©u c·ªßa m√¨nh t·∫≠p trung v√†o ng√†nh NLP (X·ª≠ l√Ω Ng√¥n ng·ªØ t·ª± nhi√™n), c·ª• th·ªÉ l√† LLMs (M√¥ h√¨nh ng√¥n ng·ªØ l·ªõn). V·ªÅ cu·ªôc s·ªëng, m√¨nh r·∫•t th√≠ch √¢m nh·∫°c. Gi·ªçng h√°t m√¨nh kh√° ·ªïn v√† ƒë√£ ƒëi tr√¨nh di·ªÖn ·ªü m·ªôt s·ªë n∆°i. Nh·∫•n v√†o [k√™nh Youtube n√†y](https://www.youtube.com/channel/UCw0K4xQPwp8wZp6rkWRcTCg) ƒë·ªÉ nghe m·ªôt v√†i b√†i h√°t m√¨nh cover ho·∫∑c t·ª± vi·∫øt :D V·ªÅ tinh th·∫ßn, l·ªëi suy nghƒ© c·ªßa m√¨nh ƒë∆∞·ª£c ·∫£nh h∆∞·ªüng l·ªõn t·ª´ [Thi·ªÅn s∆∞ Th√≠ch Nh·∫•t H·∫°nh](https://plumvillage.org/thich-nhat-hanh/), ƒë·∫∑c bi·ªát l√† qua cu·ªën s√°ch ƒê∆∞·ªùng X∆∞a M√¢y Tr·∫Øng ([en](https://terebess.hu/zen/mesterek/Thich%20Nhat%20Hanh%20-%20Old%20Path%20White%20Clouds.pdf),[vi](https://thuvienhoasen.org/images/file/3GfDvp1G0QgQAHtP/duong-xua-may-trang.pdf)). B√™n c·∫°nh ƒë√≥, t·ª´ th√°ng 02/2023 ƒë·∫øn th√°ng 04/2024, m√¨nh tham gia Ban c√°n s·ª± c·ªßa [H·ªôi sinh vi√™n VN t·∫°i HK](https://www.facebook.com/profile.php?id=100087606602683) v·ªõi vai tr√≤ Tr∆∞·ªüng ban t·ªï ch·ª©c s·ª± ki·ªán.

Cu·ªëi c√πng, b·∫°n c√≥ th·ªÉ xem nh·ªØng th√¥ng tin li√™n quan ƒë·∫øn h·ªçc thu·∫≠t c·ªßa m√¨nh t·∫°i [ƒë√¢y](https://dovanquyet.github.io/academic), v√† nh·ªØng b√†i vi·∫øt blog li√™n quan ƒë·∫øn cu·ªôc s·ªëng h√†ng ng√†y t·∫°i [ƒë√¢y](https://dovanquyet.github.io/posts/vi/chuyen-hang-ngay).

